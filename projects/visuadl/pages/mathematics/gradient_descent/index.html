<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VISUADL - Gradient Descent</title>
    <link rel="stylesheet" href="../../../assets/css/global.css">
    <link rel="stylesheet" href="./styles.css">

    <!-- Anime.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" defer></script>

    <!-- MathJax for Mathematical Rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Caveat:wght@400..700&family=Reenie+Beanie&display=swap"
        rel="stylesheet">
</head>

<body>
    <div class="layout__content_page">
        <!-- Back button to return to main page -->
        <div class="back-button" style="position: fixed; top: 10px; left: 10px;">
            <button onclick="window.location.href='../../../../index.html'"
                style="padding: 10px 20px; background-color: var(--color-primary); color: white; border: none; border-radius: 5px; cursor: pointer;">
                Back to Main Page
            </button>
        </div>
        <header class="layout__header__content_page">
            <h1 class="hero_title__content_page">Exploring Gradient Descent</h1>
            <p class="hero_slogan__content_page"><em>Optimizing Functions Step by Step</em></p>
        </header>

        <main class="layout__main">
            <section class="gradient-descent-section">
                <h2>Introduction to Gradient Descent</h2>
                <p><span class="highlight">Gradient descent</span> is an optimization algorithm used to minimize a
                    function by iteratively moving towards the steepest descent, as defined by the negative of the
                    gradient. It is widely used in machine learning and deep learning to optimize model parameters.</p>

                <h2>Mathematical Formulation</h2>
                <p>The update rule for gradient descent is:</p>
                <p>$$ \theta^{(t+1)} = \theta^{(t)} - \eta \nabla_\theta J(\theta) $$</p>
                <p>where:
                <ul>
                    <li>\( \theta \) represents the parameters to be optimized.</li>
                    <li>\( \eta \) is the <span class="highlight">learning rate</span>, a hyperparameter that controls
                        the step size of each update.</li>
                    <li>\( J(\theta) \) is the <span class="highlight">cost function</span> that measures the error of
                        the model.</li>
                </ul>
                </p>

                <h2>Simple Example</h2>
                <p>Consider minimizing \( f(x) = x^2 \). The gradient is \( \nabla f(x) = 2x \). The update rule
                    becomes:</p>
                <p>$$ x^{(t+1)} = x^{(t)} - \eta \cdot 2x^{(t)} $$</p>
                <p>This process iteratively reduces \( x \) towards zero, the minimum of \( f(x) \).</p>

                <h2>Intermediate Example</h2>
                <p>Let's minimize \( f(x, y) = x^2 + y^2 \). The gradients are \( \nabla f(x) = 2x \) and \( \nabla f(y)
                    = 2y \).</p>
                <p>The update rules are:</p>
                <p>$$ x^{(t+1)} = x^{(t)} - \eta \cdot 2x^{(t)} $$</p>
                <p>$$ y^{(t+1)} = y^{(t)} - \eta \cdot 2y^{(t)} $$</p>
                <p>This process iteratively reduces both \( x \) and \( y \) towards zero, the minimum of \( f(x, y) \).
                </p>

                <h2>Advanced Example</h2>
                <p>Consider a more complex function \( f(x, y) = 3x^2 + 4xy + y^2 \). The gradients are:</p>
                <p>$$ \nabla f(x) = 6x + 4y $$</p>
                <p>$$ \nabla f(y) = 4x + 2y $$</p>
                <p>The update rules are:</p>
                <p>$$ x^{(t+1)} = x^{(t)} - \eta (6x^{(t)} + 4y^{(t)}) $$</p>
                <p>$$ y^{(t+1)} = y^{(t)} - \eta (4x^{(t)} + 2y^{(t)}) $$</p>
                <p>This process iteratively adjusts both \( x \) and \( y \) to find the minimum of the function.</p>

                <h2>Visualizing Gradient Descent</h2>
                <p>Imagine standing on a hill and trying to find the lowest point. Gradient descent is like taking steps
                    downhill, always moving in the direction of the steepest descent. The learning rate determines the
                    size of each step.</p>

                <h2>Applications in Real Life</h2>
                <p>Gradient descent is used in various fields, including machine learning, economics, and physics. In
                    machine learning, it is used to train models by minimizing the error between predicted and actual
                    values. In economics, it can be used to optimize cost functions for better decision-making.</p>
            </section>
        </main>
    </div>
</body>

</html>